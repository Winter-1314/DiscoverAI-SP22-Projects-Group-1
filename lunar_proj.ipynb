{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Lunar Lander Environment**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**About the problem**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The task is to build an agent that can safely land a lunar rover by controlling its thrusters and main engine"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Import libraries**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Avoid reinstalling packages that are available on edstem\n",
                "if not os.getenv(\"ED_COURSE_ID\"):\n",
                "    !pip install tensorflow gym keras keras-rl2\n",
                "\n",
                "import sys\n",
                "sys.path.append('./.local/lib/python3.9-packages')\n",
                "import gym\n",
                "import random"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Set up the environment**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "# use gym's make method to generate the lunar lander environment\n",
                "env = gym.make('LunarLander-v2')\n",
                "\n",
                "# extract states and actions from environment\n",
                "states = env.observation_space.shape[0]\n",
                "actions = env.action_space.n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Testing Random Actions**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Episode:1 Score:-106.30324357113311\nEpisode:2 Score:-247.77462608429434\nEpisode:3 Score:-477.435013064628\nEpisode:4 Score:-123.12532936816025\nEpisode:5 Score:-315.50604237601135\nEpisode:6 Score:-108.71368564720947\nEpisode:7 Score:-107.97177062799771\nEpisode:8 Score:-155.33190309144462\nEpisode:9 Score:-76.99679799523831\nEpisode:10 Score:-280.4376710114186\n"
                }
            ],
            "source": [
                "# Trigger Ed's X display\n",
                "#!xdpyinfo\n",
                "\n",
                "episodes = 10\n",
                "# Repeat process 10 times\n",
                "for episode in range(1, episodes+1):\n",
                "    # Each time, reset the environment\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    score = 0 \n",
                "    \n",
                "    while not done:\n",
                "        # render the environment so that it remains visible on the screen\n",
                "        env.render()  \n",
                "        # use action sample space    \n",
                "        action = env.action_space.sample()\n",
                "        # apply the action to the environment and collect feedback\n",
                "        n_state, reward, done, info = env.step(action) \n",
                "        # Add the reward to the cummulative score\n",
                "        score+=reward \n",
                "    # End of loop: print out the maximum score\n",
                "    print('Episode:{} Score:{}'.format(episode, score))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Create a Deep Learning Model with Keras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import dependencies needed for this step from numpy and keras\n",
                "import numpy as np\n",
                "from keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Flatten\n",
                "from tensorflow.keras.optimizers import Adam"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function that builds a model so that we can reuse it multiple times\n",
                "# To build a model, the function needs the available states and actions\n",
                "def build_model(states, actions):\n",
                "    model = Sequential()\n",
                "    model.add(Flatten(input_shape=(1,states)))\n",
                "    model.add(Dense(24, activation='relu'))\n",
                "    model.add(Dense(24, activation='relu'))\n",
                "    model.add(Dense(actions, activation='linear'))\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 8)                 0         \n                                                                 \n dense (Dense)               (None, 24)                216       \n                                                                 \n dense_1 (Dense)             (None, 24)                600       \n                                                                 \n dense_2 (Dense)             (None, 4)                 100       \n                                                                 \n=================================================================\nTotal params: 916\nTrainable params: 916\nNon-trainable params: 0\n_________________________________________________________________\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2022-04-25 08:22:12.963218: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-04-25 08:22:12.964368: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
                }
            ],
            "source": [
                "# Create an instance of a model by calling the build_model function\n",
                "model = build_model(states, actions)\n",
                "# TODO: Write code to inspect the built model by outputting the summary\n",
                "model.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Build Agent with Keras-RL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import dependencies to build an agent\n",
                "from rl.agents import DQNAgent\n",
                "from rl.policy import BoltzmannQPolicy\n",
                "from rl.memory import SequentialMemory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function to build a DQN agent given the model and the set of actions\n",
                "def build_agent(model, actions):\n",
                "    policy = BoltzmannQPolicy()\n",
                "    memory = SequentialMemory(limit=50000, window_length=1)\n",
                "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
                "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
                "    return dqn"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use the DQN agent to train the reinforcement learning model.\n",
                "\n",
                "Note that this step takes a few minutes. Move to the next steps after the button on the left changes from 'stop' to show that the run is complete. Do not worry about the 'too much output' warning halfways through the run.\n",
                "\n",
                "**To test this step**, please use the *Run All* button instead of running this section alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'Sequential' object has no attribute '_compile_time_distribution_strategy'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m\u003ccell line: 4\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m dqn \u001b[38;5;241m=\u001b[39m build_agent(model, actions)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m----\u003e 4\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmae\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dqn\u001b[38;5;241m.\u001b[39mfit(env, nb_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
                        "File \u001b[0;32m/usr/lib/python3.10/site-packages/rl/agents/dqn.py:167\u001b[0m, in \u001b[0;36mDQNAgent.compile\u001b[0;34m(self, optimizer, metrics)\u001b[0m\n\u001b[1;32m    164\u001b[0m metrics \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [mean_q]  \u001b[38;5;66;03m# register default metrics\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# We never train the target model, hence we can set the optimizer and loss arbitrarily.\u001b[39;00m\n\u001b[0;32m--\u003e 167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m \u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_model_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "File \u001b[0;32m/usr/lib/python3.10/site-packages/rl/util.py:16\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, custom_objects)\u001b[0m\n\u001b[1;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mget_config(),\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m clone \u001b[38;5;241m=\u001b[39m model_from_config(config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects)\n\u001b[0;32m---\u003e 16\u001b[0m clone\u001b[38;5;241m.\u001b[39mset_weights(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
                        "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/training_v1.py:157\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    151\u001b[0m   \u001b[38;5;124;03m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m      A flat list of Numpy arrays.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m   strategy \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m--\u003e 157\u001b[0m               \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_time_distribution_strategy\u001b[49m)\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m strategy:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_compile_time_distribution_strategy'"
                    ]
                }
            ],
            "source": [
                "# Create an instance of an agent \n",
                "dqn = build_agent(model, actions)\n",
                "# Compile the model\n",
                "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
                "# Fit the model\n",
                "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
